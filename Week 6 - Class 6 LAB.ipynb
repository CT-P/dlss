{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce480f3d",
   "metadata": {},
   "source": [
    "# Week 6 - Class 6\n",
    "\n",
    "This notebook have the most important exercises for theoretical classes 4 and 5, which covers chapter 2,3,4, and 5 from the book. The labs from https://github.com/fchollet/deep-learning-with-python-notebooks are a guide for this notebook, mainly the labs 2,3,4,5. \n",
    "\n",
    "It will be covered:\n",
    "### Part 1:\n",
    "- Basic neural network\n",
    "- Selecting data\n",
    "- Tensor operations\n",
    "- Full implementation of a NN in Keras (not mandatory)\n",
    "### Part 2:\n",
    "- Linear classifier (not mandatory)\n",
    "- Movies Reviews - binary classsification-\n",
    "- Houses price - regression and k-fold validation-\n",
    "### Part 3:\n",
    "- Machine Learning flow: overfitting and generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0044bb",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d00a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d87bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d3a01",
   "metadata": {},
   "source": [
    "1.1 \n",
    "Check the dimensions of train and test images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d3e146",
   "metadata": {},
   "outputs": [],
   "source": [
    "<your-answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f4bec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "digit = train_images[4]\n",
    "plt.imshow(digit, cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdc2ce5",
   "metadata": {},
   "source": [
    "- Basic Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f68c04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(512, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173bf628",
   "metadata": {},
   "source": [
    "1.2\n",
    "Make model prediction for the first 10 test images\n",
    "\n",
    "Check if the prediction is the same as the label and plot the test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c01ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the model\n",
    "<your-answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348a8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the test label\n",
    "<your-answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the image\n",
    "<your-answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5abb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(f\"test_acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070c48e",
   "metadata": {},
   "source": [
    "- Selecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "batch = train_images[128 * n :  128 * (n + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c88e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BatchGenerator:\n",
    "    def __init__(self, images, labels, batch_size=128):\n",
    "        assert len(images) == len(labels)\n",
    "        self.index = 0\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.num_batches = math.ceil(len(images) / batch_size)\n",
    "\n",
    "    def next(self):\n",
    "        images = self.images[self.index : self.index + self.batch_size]\n",
    "        labels = self.labels[self.index : self.index + self.batch_size]\n",
    "        self.index += self.batch_size\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017cbbd",
   "metadata": {},
   "source": [
    "- Tensor operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_vector_dot(x, y):\n",
    "    z = np.zeros(x.shape[0])\n",
    "    for i in range(x.shape[0]):\n",
    "        z[i] = naive_vector_dot(x[i, :], y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_matrix_dot(x, y):\n",
    "    assert len(x.shape) == 2\n",
    "    assert len(y.shape) == 2\n",
    "    assert x.shape[1] == y.shape[0]\n",
    "    z = np.zeros((x.shape[0], y.shape[1]))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(y.shape[1]):\n",
    "            row_x = x[i, :]\n",
    "            column_y = y[:, j]\n",
    "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f214a32",
   "metadata": {},
   "source": [
    "1.3\n",
    "Apply naive_matrix_dot to the x and y tensors (build the y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ae3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[0., 1.],\n",
    "             [2., 3.],\n",
    "             [4., 5.]])\n",
    "\n",
    "<your-answer>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5115d120",
   "metadata": {},
   "source": [
    "1.4 \n",
    "Complete the naive_relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a7e1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_relu(x):\n",
    "    assert len(x.shape) == 2\n",
    "    x = x.copy()\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            <your-answer>\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca6540",
   "metadata": {},
   "source": [
    "- Full implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class NaiveDense:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.activation = activation\n",
    "\n",
    "        w_shape = (input_size, output_size)\n",
    "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
    "        self.W = tf.Variable(w_initial_value)\n",
    "\n",
    "        b_shape = (output_size,)\n",
    "        b_initial_value = tf.zeros(b_shape)\n",
    "        self.b = tf.Variable(b_initial_value)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "        return [self.W, self.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c47fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveSequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        x = inputs\n",
    "        for layer in self.layers:\n",
    "           x = layer(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def weights(self):\n",
    "       weights = []\n",
    "       for layer in self.layers:\n",
    "           weights += layer.weights\n",
    "       return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cd4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveSequential([\n",
    "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
    "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
    "])\n",
    "assert len(model.weights) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151a6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_training_step(model, images_batch, labels_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images_batch)\n",
    "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            labels_batch, predictions)\n",
    "        average_loss = tf.reduce_mean(per_sample_losses)\n",
    "    gradients = tape.gradient(average_loss, model.weights)\n",
    "    update_weights(gradients, model.weights)\n",
    "    return average_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    for g, w in zip(gradients, weights):\n",
    "        w.assign_sub(g * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904ece80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "def update_weights(gradients, weights):\n",
    "    optimizer.apply_gradients(zip(gradients, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e814f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, images, labels, epochs, batch_size=128):\n",
    "    for epoch_counter in range(epochs):\n",
    "        print(f\"Epoch {epoch_counter}\")\n",
    "        batch_generator = BatchGenerator(images, labels)\n",
    "        for batch_counter in range(batch_generator.num_batches):\n",
    "            images_batch, labels_batch = batch_generator.next()\n",
    "            loss = one_training_step(model, images_batch, labels_batch)\n",
    "            if batch_counter % 100 == 0:\n",
    "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ae2c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype(\"float32\") / 255\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype(\"float32\") / 255\n",
    "\n",
    "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29fadb4",
   "metadata": {},
   "source": [
    "1.5 \n",
    "Organize the code of the full implementation in modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4163052",
   "metadata": {},
   "source": [
    "- Liner classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e0788",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_class = 1000\n",
    "\n",
    "negative_samples = np.random.multivariate_normal(\n",
    "    mean=[0, 3],\n",
    "    cov=[[1, 0.5],[0.5, 1]],\n",
    "    size=num_samples_per_class)\n",
    "    \n",
    "positive_samples = np.random.multivariate_normal(\n",
    "    mean=[3, 0],\n",
    "    cov=[[1, 0.5],[0.5, 1]],\n",
    "    size=num_samples_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking the two classes into an array with shape (2000, 2)\n",
    "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the corresponding targets (0 and 1)\n",
    "targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=\"float32\"),\n",
    "                     np.ones((num_samples_per_class, 1), dtype=\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a89759",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed79698",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(1)])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "#or:\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
    "              loss=keras.losses.MeanSquaredError(),\n",
    "              metrics=[keras.metrics.BinaryAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a19e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_permutation = np.random.permutation(len(inputs))\n",
    "shuffled_inputs = inputs[indices_permutation]\n",
    "shuffled_targets = targets[indices_permutation]\n",
    "\n",
    "num_validation_samples = int(0.3 * len(inputs))\n",
    "val_inputs = shuffled_inputs[:num_validation_samples]\n",
    "val_targets = shuffled_targets[:num_validation_samples]\n",
    "training_inputs = shuffled_inputs[num_validation_samples:]\n",
    "training_targets = shuffled_targets[num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d26a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    training_inputs,\n",
    "    training_targets,\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    validation_data=(val_inputs, val_targets)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93f9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the batch size needed on predictions\n",
    "predictions = model.predict(val_inputs, batch_size=128)\n",
    "print(predictions[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe1eb2",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e72a20",
   "metadata": {},
   "source": [
    "- Movies Reviews - binary classsification-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c899f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d9b63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtions\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i, j] = 1.\n",
    "    return results\n",
    "    \n",
    "#data wrangling (from last lab)\n",
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22e7533",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44517a7",
   "metadata": {},
   "source": [
    "2.1 \n",
    "Build a model with:\n",
    "\n",
    "    - 3 sequential dense layers (2 relu and last one sigmoid)\n",
    "    \n",
    "    - Dense layer size is 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843abbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#see the example of a basic neural network in this notebook (1.1)\n",
    "\n",
    "model = <your-answer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8e171b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=<your-answer>,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ffe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b1112",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "                    partial_y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e04a9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b388c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2045f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1cf09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1194380d",
   "metadata": {},
   "source": [
    "- Houses price - regression and k-fold validation-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf102d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cfed77",
   "metadata": {},
   "source": [
    "1. Normalizing the data\n",
    "\n",
    "Notice the training and testing normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6a3c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea84b5c9",
   "metadata": {},
   "source": [
    "2. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc812e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808291a0",
   "metadata": {},
   "source": [
    "3. Validate (K fold validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e6ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which variable is x1 and x2? Choose their values \n",
    "\n",
    "x1 = <your-answer>\n",
    "num_val_samples = len(train_data) // k\n",
    "x2 = <your-answer>\n",
    "all_scores = []\n",
    "for i in range(x1):\n",
    "    print(f\"Processing fold #{i}\")\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    model = build_model()\n",
    "    model.fit(partial_train_data, partial_train_targets,\n",
    "              epochs=x2, batch_size=16, verbose=0)\n",
    "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "    all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d23430",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "all_mae_histories = []\n",
    "for i in range(x1):\n",
    "    print(f\"Processing fold #{i}\")\n",
    "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "    partial_train_data = np.concatenate(\n",
    "        [train_data[:i * num_val_samples],\n",
    "         train_data[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    partial_train_targets = np.concatenate(\n",
    "        [train_targets[:i * num_val_samples],\n",
    "         train_targets[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "    model = build_model()\n",
    "    history = model.fit(partial_train_data, partial_train_targets,\n",
    "                        validation_data=(val_data, val_targets),\n",
    "                        epochs=x2, batch_size=16, verbose=0)\n",
    "    mae_history = history.history[\"val_mae\"]\n",
    "    all_mae_histories.append(mae_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61606cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_mae_history = [\n",
    "    np.mean([x[i] for x in all_mae_histories]) for i in range(x2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb1b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a78a1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_mae_history = average_mae_history[10:]\n",
    "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation MAE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a27cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(train_data, train_targets,\n",
    "          epochs=130, batch_size=16, verbose=0)\n",
    "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is this a good MAE? \n",
    "test_mae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688f101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa8b301",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad14bcd0",
   "metadata": {},
   "source": [
    "Machine Learning flow: overfitting and generalization\n",
    "\n",
    "1. Have a baseline with a very simplistic model \n",
    "2. Find a simple model that overfitts\n",
    "3. Apply generalization techniques, such as:\n",
    "    - Learning rate\n",
    "    - L1 and L2 regularization\n",
    "    - Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4a6198",
   "metadata": {},
   "source": [
    "3.1 BASELINE: Apply a logistic regression for the MNIST problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954e1fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the MNIST dataset\n",
    "<your-answer>\n",
    "(train_images, train_labels), _ = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eb166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1eab97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the tensors to 2D\n",
    "<your-answer>\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "\n",
    "# scale it by the maximum pixel value\n",
    "<your-answer>\n",
    "train_images = train_images.astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd67f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a keras model of multinomial logistic regression\n",
    "# http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/\n",
    "\n",
    "#<your-answer>\n",
    "model = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(1.),\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "              \n",
    "history_small_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)\n",
    "\n",
    "# run again with LR=1e-2, compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1a4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# your y is the validation loss\n",
    "# see the varianles of history_small_model.history.keys() and choose the right one\n",
    "val_loss = history_small_model.history[<your-answer>]\n",
    "#your x is the number of epochs\n",
    "epochs = range(1, 21)\n",
    "#use the plot funtion to have the validation loss per epoch\n",
    "<your-answer>\n",
    "\n",
    "plt.plot(epochs, val_loss, \"b--\",\n",
    "         label=\"Validation loss\")\n",
    "         \n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83dca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(96, activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "              \n",
    "history_large_model = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9794cc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss = history_large_model.history['val_loss']\n",
    "#your x is the number of epochs\n",
    "epochs = range(1, 21)\n",
    "#use the plot funtion to have the validation loss per epoch\n",
    "#<your-answer>\n",
    "\n",
    "plt.plot(epochs, val_loss, \"b--\",\n",
    "         label=\"Validation loss\")\n",
    "         \n",
    "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fbc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(96, \n",
    "                kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                activation=\"relu\"),\n",
    "    layers.Dense(96, \n",
    "                kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001),\n",
    "                activation=\"relu\"),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "              \n",
    "history_large_modelReg = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6acbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(96,activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(96,activation=\"relu\"),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "              \n",
    "history_large_modelReg = model.fit(\n",
    "    train_images, train_labels,\n",
    "    epochs=20,\n",
    "    batch_size=128,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56651913",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.16 ('tf-py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "5395b24c011b6be8266d8f5b848fa72ba88c74a4bc2c01f0d440a0d322d0c4eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
