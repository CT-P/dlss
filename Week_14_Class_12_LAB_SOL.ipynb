{"cells":[{"cell_type":"markdown","metadata":{"id":"uacj685SvQM8"},"source":["# Week 14-  Class 12\n","This notebook have the most important exercises for theoretical class, which covers chapter 11 from the book. The labs from https://github.com/fchollet/deep-learning-with-python-notebooks are a guide for this notebook, mainly the labs from chapter 11 and 12"]},{"cell_type":"markdown","metadata":{"id":"thupIQWrvQM-"},"source":["####1. Downloading the data"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"w1OzGCiZvQM-","executionInfo":{"status":"ok","timestamp":1684916390570,"user_tz":-120,"elapsed":18451,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"daf54f25-a9ae-4b03-e33c-13d1ad19c92f","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  16.1M      0  0:00:04  0:00:04 --:--:-- 18.1M\n"]}],"source":["!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz\n","!rm -r aclImdb/train/unsup"]},{"cell_type":"markdown","metadata":{"id":"CqizRtEsvQM-"},"source":["####2. Creating the dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"vlCI8oZzvQM_","executionInfo":{"status":"ok","timestamp":1684910833990,"user_tz":-120,"elapsed":3477,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"5105a6c5-eeae-4b10-ccf9-a4103134b9b9","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 12800 files belonging to 2 classes.\n","Found 12200 files belonging to 2 classes.\n","Found 25000 files belonging to 2 classes.\n"]}],"source":["import os, pathlib, shutil, random\n","from tensorflow import keras\n","\n","batch_size = 32\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","\n","for category in (\"neg\", \"pos\"):\n","    #os.makedirs(val_dir / category)\n","    files = os.listdir(train_dir / category)\n","    random.Random(1337).shuffle(files)\n","    #20% for validation\n","    num_val_samples = int(0.2 * len(files))\n","    #select last num_val_samples (files) from array\n","    val_files = files[-num_val_samples:]\n","    #move last 20% of training files to validation\n","    for fname in val_files:\n","        shutil.move(train_dir / category / fname,\n","                    val_dir / category / fname)\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\", batch_size=batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/val\", batch_size=batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")\n","text_only_train_ds = train_ds.map(lambda x, y: x)"]},{"cell_type":"code","source":["for x, y in train_ds:\n","  print(x.shape)\n","  print ('\\n')\n","  print(y)\n","  break"],"metadata":{"id":"WGI7z-I9wX9c","executionInfo":{"status":"ok","timestamp":1684910011292,"user_tz":-120,"elapsed":10,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"9007f9f3-5643-4ef0-ab6b-68dbb850bf64","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[b'I think that it was just pointless to produce a second part of a movie like \"My Girl\". \"My Girl\" was a very good movie but it is ridiculous making a second part of a movie in which one of the main characters (Macaulay Culkin as Thomas J.) dies. The story was over after the first movie. I wonder why someone tried to find a way to make the story going on. That was senseless!'\n"," b'The combination of reading the Novella and viewing this film has inspired my wife and I to new levels. Recently I was pondering a statement made by the artist Thomas Kinkade in one of his inspirational books; He states: \"You and I were not designed to breathe the fetid air of five o\\'clock traffic. Nor do I think God had banal television programs, media hype, worthless purchases, and soul pollution in mind when he created the universe...\" I hadn\\'t seen \"A river runs through it\" in a couple of years, but after pondering Kinkade\\'s statement something drew me to watch the film with a spiritual eye. I watched it and saw a whole new world to the film and it inspired me to read the book (a must read). I have always been frustrated in Southern California but somehow got caught up in its materialistic society. The film really puts into perspective of how we should really experience God\\'s creations. A combination of Macleans story and my desire to move back to the Northwest has driven me to move to Montana. I want my future kids to be able to rome the landscape, go fly-fishing with me, ride horses into nothing but open land and serene lakes set in the mountainside. A place where you seldom worry about crime. I look around SoCal and all I see is shopping malls, rude snarling people in their Mercedez Bens, miles of vehicles on congested freeways, gangs, racial turmoil on the verge of violent eruption, and everyone skeptical of each others intentions.<br /><br />Anyway the movie is very inspiring with brilliant acting and a deep story about the fragile connections of loved ones. There is a lot of deep thinking in this film. The scenery is worth seeing alone and actually helps relieve tension. You should finish this film relaxed yet full of insights to your own life. It takes a compassionate, intelligent, and spiritual person to really grasp the meaning. If you don\\'t understand the art of cinema and how a director achieves his goals through dialogue, tone, light, colour, scenery, camera angles/movement, etc. Then this film is probably not for the crowd that thinks \"The Fast and the Furious\" is the greatest film. Granted it was entertaining but shallow.<br /><br />The bottom line: This film helps to realize that life is not about how much money you have or what things you posses. Rather it is about your relationships with family and friends and the experiences you share together. QUALITY NOT QAUNTITY'\n"," b\"I recently viewed Manufactured Landscapes at the Seattle International Film Festival. I was drawn to the movie as a photographer because I'm both familiar and a fan of Burtynsky's work. While I believe the movie does a good job getting it's message across, I couldn't help but feel that it was made as a complete afterthought to the photographs and subsequent popular book by Burtynsky. Obviously one reason for this is the extensive use of still photographs featuring zooms and pans across them. While this is a good effect when used economically, I felt like 75% of the movie was just stills from Burtynsky's book (which I already own). That's probably an exaggeration, but that's how I felt. If you own the book or are familiar with his work you might be better off skipping this one.\"\n"," b'I will be short...This film is an embarrassment to everyone except its cinematographer. The very fact that it is a critique of the sex tourism industry seems valid until we are \"treated\" to a lingering dance scene. The plot is ridiculous no one except the most ardent fan of BAD horror will get anything out of it. And for the love of God please stop saying this film is a tale of innocence lost or even of female empowerment because it is quite clearly not (childish fumbling lesbians, what the hell?). this was by far the worst film at the Edinburgh festival (that i saw anyway), someone even collapsed halfway through the film probably because they couldn\\'t take any more of it. this may seem like an overly critical rant but i genuinely cannot find a redeeming feature of this film except for perhaps if you take it as pure comedy. In short this film is best watched on a cocktail of class A drugs.'\n"," b'This is the last time I rent a video without checking in at the IMDB reviews. The Limey is directed by Steven Soderbergh who also wrote wrote the truly awful Nightwatch with Ewan Macgregor and directed such trash as Out of Sight with the anti-talented Jennifer Lopez. Terence Stamp is a fine actor and it is a shame he involved himself in such a bad film. There is frequent confusing editing that seems like it was a last minute decision in order to make up for the lack or story, filming and just plain common sense. This film does everything wrong. What were they thinking?'\n"," b'<br /><br />As usual, I was really looking forward to a new TV/film on a favourite subject of mine - makes a nice change from a *strangely familiar* documentary about Kursk or Stalingrad on the History Channel.<br /><br />I avidly looked forward to Pearl Harbour and Enemy at the Gates - but was rudely brought down to earth with the realisation of the malevolent, stupid-ifying power of Hollywood - and its ability to spend an absolute fortune on tripe.<br /><br />So yet again I got excited about \\'The Rise of Evil\\', especially as I heard that Ian Kershaw was involved, as I\\'ve enjoyed his books. I can see why he quit.<br /><br />To quote some guy responsible for this rubbish:<br /><br />\"The Kershaw book was an academic piece,\" he said. \"It was<br /><br />quite dry. We needed more incidents.\" <br /><br />Incidents? Are they totally nuts? Hitler\\'s life cannot be said to be without \\'incident\\' - yes Kershaw\\'s two volume Hitler biographies were long and detailed, but they were supposed to be.<br /><br />The thesis behind \\'Rise of Evil\\' seems to be:<br /><br />Hitler was a very bad man - no he was a VERY bad man, who HATED jews, and just in case you miss this, we\\'re going to emphasise the fact in EVERY scene in the film.<br /><br />There was no effort whatsoever to try and explain the mood of the time, and why Hitler may have adopted the views and strategy he did. Needless to say - unlike the generally excellent \\'Nazis - A Warning from History\\' - this film neglected to point out the fact that nearly all of the leaders of the Munich communist rising were Jewish, and that this may have coloured his views on the subject - and his axiomatic linking of the jews with Bolshevism - an absolutely crucial aspect to understanding much of the Nazi era.<br /><br />But there was not much understanding to be done - the film-makers weren\\'t going to go there, so we just got all the stuff we knew about anyway. We certainly don\\'t get the fascinating fact that Kershaw alludes to, which has Hitler briefly being a socialist/communist immediately after WW1. That would of course be far too complex for the film to handle, and might even detract from the relentless \\'he was very bad\\' mantra which bangs away incessantly.<br /><br />We know he was a bad man. However, we also know that he was a mesmerising figure both as a public speaker and in more private situations. He could be polite and even sympathetic, and of course espoused some views like vegetarianism, anti-alcohol and anti-smoking that many Guardian readers could agree with. He was also famously fond of animals, hence why that wholly invented dog-flogging scene was so absurd.<br /><br />He was also, from all the accounts I\\'ve seen, a brave soldier in WW1. Whilst we saw him with his Iron Cross, we never get to see how he won it (acts of bravery were not in the script, needless to say). We also get no insight whatsoever into why he was so fired up by his war experiences, whilst Sassoon, Owen, Brook, Remarque and so many others found it so repellent an experience. And again, like the point above re the jewish/bolshevik link, this is vital to anyone\\'s understanding about the subject. Why did he love war so much? Why did he think it was always a good idea, despite massive evidence to the contrary? Why didn\\'t he care about his colleagues who died? Or maybe he did - but still drew the wrong conclusions.<br /><br />This film certainly didn\\'t have anything of any interest to say on this either.<br /><br />As all too often these days, the film is a classic example of \\'making history relevant to the present\\' and inventing stuff or leaving awkward facts out to fit in with \\'the present\\' - which all too often is to cater to the lowest common denominator, where you don\\'t trust your audience an inch, so you just ram stuff down their throats, knowing (sadly correctly) that you\\'ll always get away with it because there are so many dumb fools in the world.<br /><br />History is really about making us relevant to the past and seeing how it colours our present, for better and for worse. This rubbish was a great opportunity, lost again. They spent millions on it, and the locations and large scenes were impressive, but told us nothing at all we didn\\'t know already, and promoted no understanding of this dark period in human history.<br /><br />WT'\n"," b'It appears that there\\'s no middle ground on this movie! Most of it takes place in a dream and, like most dreams, it\\'s often foolish and illogical. It\\'s also a gorgeous production with some great songs and fine performances, especially by our angel.<br /><br />Jeanette\\'s deadpan, unknowing insults and various other faux pas at the dream reception are hilarious, and her jitterbug with Binnie Barnes is a surprise and a delight. At one point, she gets to sing a snippet from Carmen, followed by the final trio of Faust (holding a lapdog, for some strange reason), then \"Aloha Oe\" on the beach! <br /><br />It\\'s a surreal comedy--tremendously entertaining if you can get into the groove.'\n"," b'I watched to movie today and it just blew my mind away. It is a real masterpiece of art and I don\\'t understand why most of the people think it\\'s garbage. The main idea of the movie - take your ego away and then you will have true power! This was the main battle at the end of the movie and Guy Ritchie has shown that in a magnificent way. \"The greatest enemy will hide in the last place you will ever look\" - do you remember this from the movie? Because our true enemy is in us - it is our ego... That voice that always tells us that we are important, that gives us our pride, that tells us not to give, but only to take, that creates our aggression, that wants to be in control, that creates all the negative feelings and thoughts. GR expressed this idea in an astonishing way and has shown that the only way to gain true control is when you loose control and you just let go of your personal importance. A superb movie!'\n"," b\"I think that most of the folks who have posted comments on this movie don't understand how to watch a movie and/or have little sense of elegance. First, to assess a movie you need to understand the extent to which everything in the film works together. Modern sensibilities demand great drama. No, I don't mean great setting of characters and plots, but they seem to demand emotional trajectories that are greatly tragic or greatly comedic. This is a subtle movie. Its beauty lies in its subtlety (not to be confused with simplicity). Neither the story nor the characters are simple in this movie. It is a beautifully filmed movie that makes the most of combining sensuousness, politics, human weakness, venality...you name it. The world it's set in would be alien and not understood today...a world where if you have it you have to flaunt it NOW and LOUDLY, even if you only think you have it.<br /><br />Many people today don't understand that Victorian society wasn't really Victorian as people understand that term today.<br /><br />This movie helps set the record straight.\"\n"," b\"I'm a huge classic film buff, but am just getting in to silent movies. A lot of silent films don't hold my attention, but Show People is a notable exception.<br /><br />Marion Davies and William Haines are simply wonderful in this picture. Davies, in particular, shows a wide range as she morphs from a giggly small town girl to a starlet who takes herself a bit too seriously.<br /><br />Show People is a fast paced film with a fantastic array of cameos by some of the biggest stars of the silent era. The movie captured my attention immediately and I actually forgot that it was a silent film. (I know that doesn't make much sense, but that's what happened.) The actors are so skilled in their craft that few dialogue cards are necessary.<br /><br />Show People is a perfect introduction to silent films. It is a fast paced, interesting film with two of the silent era's best stars. Add in the satire of Hollywood and Show People should be on the 'must see' list for all classic film buffs.\"\n"," b'Why is this show so popular? It\\'s beyond me why people like it. I think it\\'s one of the worst sitcoms out there. <br /><br />Because it\\'s so popular, I\\'ve tried more than once to watch it and I can\\'t make it through an entire episode.<br /><br />For one thing, the acting is horrible. Everybody is overacting to the point where it\\'s annoying to watch. They talk in unnatural voices, use unnatural tones, and have unnatural body language. I\\'ve seen better acting in a kindergarten school play.<br /><br />For another thing, it\\'s NOT FUNNY. The plots are dull. They\\'re not creative, intelligent, or FUNNY. Shouldn\\'t a sitcom be funny?? Why am I not laughing?<br /><br />Lastly, what is seventies about this? It\\'s about as authentic to the seventies as \"Happy Days\" was to the fifties.<br /><br />OH and what is up with Ashton Kutcher? Who cast this untalented dweeb? And now he\\'s making movies?? Oh, save us all!<br /><br />If people think this is quality television, it worries me.'\n"," b\"Please, for the love of God, don't watch it. Now saying that, I know what you're thinking, it can't be that bad can it? If everyone says it as bad as they say, I have to watch it! Don't do it! It'll be like looking at a horrible accident involving little babies and a gasoline tanker! You'll be scarred for life...the image will never leave you! I could only watch a half hour of this before becoming violently sick. The acting is the worst I've ever seen, and I've seen Barbwire!!! If you do risk ripping your eyes out and rent this movie...don't say I haven't warned you! The cover and storyline are a trap!! Zombies? Satire? Shaun of the Dead was great! This movie must be the same....right? NO!! The writing = crap directing = garbage acting = there was no acting. Still not convinced? Then forever your soul will be tormented!!!\"\n"," b\"9/11 is a classic example of cinema verite, a sort of realist documentary, in this case of New York firemen as they battle against one of the most extraordinary events of world history. It's all tiny, unobtrusive, hand-held video cameras, often betrayed by the poor quality of most of the filming (and by the director, Naudet's hand frequently wiping the screen).<br /><br />In this film, you get to know most of the firemen - Tony Benatatos, the rookie (or 'probie', in NY fireman vernacular), the Fire Chief Joseph Pfeiffer (who finds he's lost his brother later on) and a few others. There are studio interviews with most of these people throughout the film, just to emphasise the personal, reflexive nature of the events. The build-up is quite dramatic and well-done, particularly the passing-out ceremony at the Fire Department, with a few useful swish-pans and a sort of dialectical editing of the rather limited filmwork (just like Rob Reiner's A Few Good Men). Tony looks proud.<br /><br />The viewpoint and camera angle is usually from amidst the firemen, which is interesting and there is some excellent footage from inside the lobby of WTC1 while Pfeiffer and his team plan what to do next - this is classic cinema verite. There is also the eery, haunting sound of the occasional human body crashing against the portico outside. It is then that an increasingly forlorn Fire Chief Pfeiffer realises that his task is desperate and probably hopeless - and this is before WTC2 collapses. You have to give credit to Naudet for knowing which faces to film and at which moment.<br /><br />The sound of the neighbouring WTC2 collapsing is so awfully sad, poignant and terrifying that you realise what an ordeal this is for the firemen. From the lobby, it looks, feels and sounds like the end of the world and the poor firemen look so utterly bewildered and frightened. You hear an enormous rumbling, trembling maelstrom - like that of a giant, monolithic beast slowly falling to the ground after being so mortally wounded - the neighbouring tower has collapsed yet the fire team remaining in WTC1 are oblivious to this event. Where is the communication?<br /><br />This film is captivating yet the narration is amateurish and should have been avoided - cues like 'this really was a day like no other' or Naudet's frequently banal pronouncements like 'you could see fear in everybody's eyes' and 'I knew Tony was freaking out'! The film is really just one long video diary. There are no pictures from higher up the building where some of the firemen have gone. Imagine this film blended with CCTV footage from some of the rooms higher up or some of the news coverage from the day. The effect would be greater. You could even combine this story with that of Mayor Giuliani and, perhaps, the famous Cornishman Rick Riscorla who literally was many floors up acting the hero.<br /><br />I don't see much of a propaganda element in this film, as some reviewers suggest. This film is no Triumph of the Will, by Riefenstahl. Some time later the firemen drape the American flag over a nearby, surviving building overlooking what has become Ground Zero. So what?<br /><br />There are also some moments of dubious camerawork; for example, who is holding the camera when the two Naudet brothers are reunited back at the fire station? Is it staged?<br /><br />There is an excellent finish, very much in the traditon of the excellent French director Alain Resnais (Hiroshima mon amour), with two strips of light reflected in the water, shimmying.\"\n"," b'A low point in human interaction was reached by the Maysles Brothers with this film. Do remember, you who used words like \"masterpiece\"when reviewing this film, that these Maysles creeps didn\\'t just happen to drive to the Hamptons and happen to shoot film on some eccentric people. No, when they found these two poor pathetic people they then had to finance their project (and imagine what they told the money people to sell the project). Then they befriended the two extremely vulnerable women. No meeting of minds here or real consensual participation. These wretched Maysles smiled, kissed ass, did whatever they had to to get the Beales to cooperate and then exploited them as viciously as has ever been done. One would like to think that these hustlers had occasional thoughts of remorse and guilt. But the film-making process, given the preplanning, actual shooting and then editing took a lot of time and their goal had no provisions for actually relating to the Beales as human beings. An exploitation film perpetrated by the vilest of people. As time accrued their film-making reputation has been seriously stained by what they did here. Their reputation as human beings is execrable. That is what people will remember them as. Grotesque hustlers.'\n"," b'In 1958, Clarksberg was a famous speed trap town. Much revenue was generated by the Sheriff\\'s Department catching speeders. The ones who tried to outrun the Sheriff? Well, that gave the Sheriff a chance to push them off the Clarksberg Curve with his Plymouth cruiser. For example, in the beginning of the movie, a couple of servicemen on leave trying to get back to base on time are pushed off to their deaths, if I recall correctly. Then one day, a stranger drove into town. Possibly the coolest hot rodder in the world. Michael McCord. Even his name is a car name, as in McCord gaskets. In possibly the ultimate hot rod. A black flamed \\'34 Ford coupe. The colors of death, evil and hellfire. He gets picked up for speeding by the Sheriff on purpose. He checks out the lay of the land. He is the brother of one of the Sheriff\\'s victims. He knows how his brother died. The Clarksberg government is all in favor of the Sheriff. There\\'s only one way to get justice served for the killing of his brother and to fix things so \"this ain\\'t a-ever gonna happen again to anyone\": recreate the chase and settle the contest hot-rodder style to the death. He goes out to the Curve and practices. The Sheriff knows McCord knows. The race begins... This is a movie to be remembered by anyone who ever tried to master maneuvering on a certain stretch of road.'\n"," b\"Branagh is one of the few who understands the difference between a film and a play. Hamlet is probably the most faithful adaptation of Shakespeare to a film and yet is a very dynamic film, almost an action thriller. The scene of Hamlet's meeting with his father's ghost won't leave your mind.\"\n"," b'Yaitate!! Japan is a really fun show and I really like it! It was shown in our country just recently in Hero TV and ABS-CBN every 5:30. It is about Azuma Kazuma who is trying to fulfill his dream to make Japanese bread that will represent his country. He is working in the Southern Toyo branch of Pantasia and he is also helping his friend (Tsukino Azusagawa) along with other bakers (like Kawachi Kyousuke and Kanmuri Shigeru) to beat St. Pierre and take control of Pantasia. They fight other skillful bakers from many other countries and not only learn to make different kinds of bread but also learn to cook other food. It is a really funny and unique anime because they also mimic characters from other anime(like Naruto, Detective Conan and One Piece)and famous people from real life. It is one of the best works of Takashi Haschiguchi and is really a must-see for people of different ages.'\n"," b\"This is one of Disney's top five animated features, in my opinion. Cinderella was a perfect return to the full-length feature animation film (as opposed to the compilation films of the 40's), and expensive depth via the multi-plane camera returns to the film in no other way. Although Disney adapts the story somewhat liberally, you gather the idea of the era via the dress and set stylizations---a clear time period the story takes place.<br /><br />Cinderella is more mature than Snow White, and a multi-dimensional character. Actually, all of the characters are somewhat well-developed, except for the Prince--left the most flat--we know he has a sense of humor, and a great smile, but that's about all. Like Snow White, Disney has some permanent impact on the story in popular culture---in most versions of Cinderella, the stepsisters are attractive, just not as pretty as Cinderella, and their character takes away from their otherwise nice appearance.<br /><br />Favorite Disney additions: the mice! Also, appreciated the continuity--Cinderella always loses her shoe throughout the film. The addition of the homemade gown as well as the following assault from the stepsisters was always horrific as a child--I remember View Master showing this with a black background and a large red light on it! The broken slipper shows the unwillingness of evil Lady Tremaine to give up her hold over Cinderella and admit defeat---Audley would go on to characterize the most wicked of all Disney villains, satanic witch Maleficent, in Sleeping Beauty.\"\n"," b'I seriously can\\'t believe Tim Burton and Timur Bekmambetov, two people I LOVE, signed on to produce this crap. Tim Burton is a brilliant director, but to be honest I\\'ve been losing interest in him for a while since his last few movies were either remakes or adaptations. He did produce the brilliant \"Nightmare Before Christmas\", which is one I\\'ve watched multiple times, and directed movies like \"Beetlejuice\" and \"Sleepy Hollow\", which are awesome films. Bekmambetov directed 3 films that I LOVE: Night Watch, Day Watch, and Wanted. I\\'ve only seen those three of his, but they prove he\\'s an awesome director.<br /><br />Those two people producing one of the many reasons I was excited to see 9. So today I went to go see it at the theatre. I was so excited to finally have seen it. I had waited 7 months for the movie to come out.<br /><br />This movie is the first time I\\'ve walked out of a Tim Burton-related movie and said \"I enjoyed almost NONE of that\". I felt heartbroken to even have felt that way. I mean, with him and Bekmambetov at the production helm you\\'d have expected this movie to be a good watch. Right now I still can\\'t get over how let down I was by this movie. I hadn\\'t even heard of the original short film before seeing it but now, I can successfully say that this movie should have remained a short movie. Hell, Neil Blomkamp made an AWESOME full length remake of Alive in Joburg entitled District 9, what was so hard to get right about 9??? I really wanted to think this movie was awesome. I really did. But no, it failed on so many levels.<br /><br />The plot was extremely confusing and disjointed. I had no idea what was going on, let alone what it was about. Basically it\\'s about a bunch of rag doll robots trying to save the earth. Well, OK, that\\'s what I got from it. But the writing here is extremely poor. The whole film jumps around like a 6 year old with A.D.D. telling a story. There\\'s this big, giant clanky monster robot that 9 awakens, causing destruction and stuff. That\\'s the main villain. However, what else is wrong with this movie is that EVERYTHING COMES OUT OF NOWHERE. There were too many monster robots, most of which have no logical explanation behind them. They have 0 development whatsoever. I mean, that flying pterodactyl like monster just rips out of nowhere, we have no idea where it comes out of and Acker just expects us to know what it is. What was even more retarded was that snake-like creature with the strobing eyes that hypnotize. I dare you to give that description to someone else out loud and expect them not to laugh. All of the 3 people I told about it burst out laughing. Oh and it wraps victims up and sews them inside it. I\\'M. NOT. KIDDING.<br /><br />The twist in Act III is the most retarded aspect of the whole movie. So basically 9 goes back to the room he woke up in, finds this box with a hologram from the scientist in it for 9, and he tells him that the big scary machine robot was designed to bring robot life to earth, but then evil humans use it for war, and it was supposed to help protect the earth, but then the scientist gave his life to 9 so that it could help protect the world with it. And HE ONLY MENTIONS GIVING HIS LIFE TO 9. But what about the other robots? WHO GAVE THEIR LIVES TO THEM???????? This is the perfect example of poor, rushed writing. There\\'s only one of the life taking device thingy that exists so how did the other 8 get life given to them??????? The characters are not likable at all either. They risk their lives for no reason at all. The only good character is 7. 6 annoyed me with his \"GO BACK TO THE SOURCE!!!!!!\" ramblings, 1 is an overpowering idiot, 2 we don\\'t know ANYTHING about, 5 kept annoying me with his \"Are you sure...\" or \"Can I stay here instead...?\" questions. And that ending? UGH. I\\'m glad I\\'m not the only one who thinks that the ending was a huge WTF moment.<br /><br />There\\'s nothing redeeming about this turd except for its beautiful animation. Everything looks realistic and beautiful, I love the gloomy and depressing look of everything. However, beauty can\\'t save a good movie.<br /><br />While it\\'s true that this movie is very pretty looking, pretty is as pretty does, and 9 does squat. I\\'m sure Burton fans will be flocking to the theatre to see this movie without a doubt, in fact with his and Bekmambetov\\'s names being thrown around the promos, people will be flocking to the theatre to see this movie. I know I may be making a big deal out of nothing, but watching this movie made me realize how much I hate movies with unlikeable characters, nonexistent plot and just pure style over content. And this movie is one of those movies.'\n"," b'Yep, you read that right, kids. Michael Bay should\\'ve studied this film before making either of his over hyped, overlong, overly pointless \"Transformers\" movies. \"Robot Jox\" is better than both of them and it probably cost less than the \"Transformers\" crew spent on Megan Fox\\'s personal trainer.<br /><br />Thankfully, this little robotic gem, initially known mainly for being the film that bankrupted Charles and Albert Band\\'s Empire Pictures studio, seems to have developed a cult following over the years. I fondly remember watching it on VHS during its initial video release in the early 90s and though some of the Cold War-era politics/stereotypes were already out of date by that time (just the Bands\\' luck that Communism would fall while the film was sitting on a shelf waiting to be released, eh?), it\\'s still a pretty damn cool little B-Movie. They really don\\'t make\\'em like this anymore, or if they do, they go the Bay route and CGI things to unbearable proportions.<br /><br />For those who are unfamiliar, here\\'s the Robo-scoop: We\\'re somewhere in the future and after a nuclear holocaust, large scale \"wars\" have been outlawed. Disputes between nations are now settled mano-a-mano (or perhaps that should be machine-o-machine-o) by one representative from each side battling each other in giant sized Shogun Warrior style robots. Whichever \\'bot walks away from the fight wins for \"his\" side. Gary Graham (who would later go on to play Detective Sykes in the \"Alien Nation\" TV series), plays \"Achilles,\" the greatest Robot Jock in Marketplace (a.k.a. the good guys) history. Achilles has been undefeated in his previous nine Robot bouts (ten being the maximum number of battles before a \"Jock\" is retired) and at the beginning of the film he faces off against his counterpart from the \"Confederation\" (i.e. The Russkies!), the psychotic Alexander (who is the most over the top \"evil Russian\" stereotype bad guy since Dolph Lundgren\\'s infamous turn as Ivan Drago in \"Rocky IV\"). The match is called a draw when Alexander violates the rules with an illegal Robot Move at the last minute and ends up not only embarrassing Achilles, but killing a whole bunch of spectators in the bargain. A rematch is scheduled to complete the bout, but Achilles simply wants to bow out, hang up his helmet and move on with his life. Rather than violate the Spoiler Rules by revealing much more, I will simply say that there are a great deal of twists and turns, behind the scenes skullduggery, and other difficulties for Achilles and his fellow \"Jox\" before the two robotic titans clash finally once again in the finale.<br /><br />I hope I\\'m not making this movie out to be some sort of masterpiece of science fiction, because it isn\\'t. \"Robot Jox\" is just plain fun. I\\'ll grant that it is a bit higher-concept than your average B-grade sci-fi movie, and though the budgetary constraints do occasionally make themselves known (especially in the scenes involving some painfully obvious green-screen trickery), it is still the best looking movie ever to come out of the Empire/Full Moon Pictures factory. The robot fight scenes are very well done using old school stop motion/model techniques, and the sets and costumes don\\'t look half-assed in the slightest. Empire Pictures and director Stuart (\"Re-Animator\") Gordon were definitely shooting for the stars with this picture. Unfortunately it didn\\'t quite pan out for them (or the studio) but at least we got one heckuva cool little movie out of the deal. Bottom line: if you want to be aurally and visually assaulted for 2+ hours, feel free to rent a \"Transformers\" movie. By the end you\\'re likely to feel like you\\'ve spent all that time watching someone else play a video game. If you want to have a rock\\'em, sock\\'em robot good time, pick up Robot Jox instead.'\n"," b'An annoying experience. Improvised dialogue, handheld cameras for no effect, directionless plot, contrived romance, ick! to the whole mess. Ron Silver was the only real actor. Gretta Sacchi was TERRIBLE! Henry Jaglom did better with Eating which suited his style much more.'\n"," b'Enterprise is the entertainment, but it is also the forefront of Science Fiction and a positive outlook for tomorrow. With gratitude and respect Mr. Berman and Mr. Braga. I wish you well, thank you both for your service to Trek.<br /><br />Enterprise is what Trek is about...'\n"," b\"This is the worst piece of crap I have seen recently. There is nothing good about this movie. The plot is plain stupid, dialogs don't make any sense, humorous scenes never heard anything about the real humor. Actors just don't play, the worse they don't even try. The script itself is somewhat which is in the same league with Ed Wood and Uwe Boll. There is only one good thing in this flick, the fights. They are well choreographed as one would expect of the Hong Kong guys, and are the only reason to watch Prince of the Sun. Although I believe the fights are just supposed to fill the empty space so that the screenwriter didn't have to bother thinking about the storyline. However, this weak and absurd plot may prevent you from watching it to the end. Avoid it unless you are fan of the dragon lady Cynthia Rothrock.\"\n"," b'First off, anyone who thinks this sequel to William Friedkin\\'s \"The French Connection\", is superior is most definitely completely insane or moronic or both. The problem with reviewing this film is that, a.) it\\'s a sequel to a brilliant movie, which always makes watching it objectively difficult, and b.) it\\'s directed by John Frankenhimer, one of the best American directors ever, so I wanted to like it. William Friendkin was the perfect person to direct a film about drug traffic in decaying new York city, because of his documentary-like approach to the action and story, Frankenhimer on the other hand is one of the most stylish directors ever, i.e. \"The Manchurian Candidate\" and \"Seconds\", and with his \"French Connection 2\" it feels like someone trying to be gritty and not having the true understanding to pull it off. That fact that Frankenhimer was chosen to direct the sequel by Gene Hackman himself really tells a lot about Hackman\\'s understanding about the original film too. It\\'s well known Hackman hated Friedkin on the set and vowed to never work with him again, it\\'s also known he envisioned the character to be more one dimensional, loosing weight and trying to play him like a straight character. it shows you Hackman, despite being a great actor, had no idea who to make the movie and the story great. The plot point of Doyle becoming an addict is interesting, but doesn\\'t warrant the rest of the film. An unfortunate low point in Frankenhimer\\'s filmography.'\n"," b\"I am amazed with some of the reviews of this film. The only place that seems to tell the truth is RottenTomatoes.com. This film is awful. The plot is extremely lazy. It is not scary either. People out there who think that because it stars Sarah Michelle Geller it is somehow like The Grudge should forget about it. This film is more like Dark Water, except it is even more predictable and slow moving than it. I was extremely disappointed with this film. It didn't scare me nor interest me either. Let's face it , this type of plot has been flogged to death at this stage e.g. the dead trying to contact the living - Dragonfly, What Lies Beneath, Ghost Story, Dark Water, Darkness, The Changeling etc.etc. It seems to me that the only ones writing original horror films nowadays are the Japanese and the Koreans. The films that are coming out of Hollywood, like this, are cynical exercises in money making without a shred of respect for the viewer. They're just being churned out\"\n"," b\"SPOILERS 9/11 is a very good and VERY realistic documentary about the attacks on the WTC.2 French film makers who are in New York to film the actions of a NYFD are being confronted with this event and make the most of it.Before 9/11 nothing much really happens which gives the movie an even more horror like scenario. On the day of the attacks it seems like just another dull day at work but this will soon change.As one the film makers goes on the road with the firemen he films the first crashing plane,this is the only footage of the first impact.He rides with the firemen to the WTC and goes inside the building.As the second plane crashes the people understand that this is not an accident.In the next period of time we see firemen making plans to save as many people as possible,in the meanwhile we hear banging sounds,these are the sounds of people who jumped down from the tower and falling on the ground,this is the most grueling moment in the documentary.Then the tower collapses and our French friend has to run for his life,you hear him breath like a madman while he runs out of the building.Then a huge sort of sandstorm blasts over him and the screen turns black,he was very lucky to survive and now he can film the empty streets of Downtown New York. Because this documentary has got so much historical footage and because the film was ment to be something totally different this documentary will probably stay in everybody's memory.I saw the attacks live at home because I had the afternoon of,so this makes it even more realistic to watch. 10/10\"\n"," b\"As I said, the book was pretty good and this might have been a good movie if Melissa Gilbert hadn't been so horrible and unbelievable in the lead roll. What kind of accent was that suppose to be anyway? It sounded the same as her horrible Russian accent in another movie that I have seen her in. Every time she opened her mouth I cringed. It took 3 tries before I was able to watch the entire movie. Brad Johnson was good as the other lead. <br /><br />I really liked the beach location scenes. They added some much needed brightness to take your mind off of Melissa Gilberts depressing portrayal. I think they could have used San Francisco views more to their advantage though. It looked like the night scenes were actually SF, but I could be wrong. I don't recall the character in the book being this depressing. <br /><br />Please keep Melissa Gilbert out of any future movies that require an accent!\"\n"," b\"Heavenly Days commits a serious comedy faux pas: it's desperate to teach us a civics lesson, and it won't stop until we've passed the final exam. Fibber McGee and Molly take a trip to Washington, where they see the senate in action (or inaction, if you prefer), have a spat with their Senator (Eugene Palette in one of the worst roles of his career), get acquainted with a gaggle of annoying stereotypical refugee children, and meet a man on a train reading a book by Henry Wallace. Henry Wallace!! A year later, he was considered a near communist dupe, but in 1944, he was A-OK. Add in some truly awful musical moments, a whole lot of flagwaving hooey, and a boring subplot about newspaper reporters, and you've got a film that must have had Philip Wylie ready to pen Generation of Vipers 2: D.C. Boogaloo. Drastically unfun, Heavenly Days is another reminder that the Devil has all the best tunes.\"\n"," b\"Alexandre Aja's remake of The Hills Have Eyes was one of the bright spots of 2006. Not only was it a remake of a classic horror film, but it was pretty damned good too. So, nearly a year later, we are being treated to the sequel to that remake. While original scripter Wes Craven is back as producer and co scripted, this film just fails to rise to the level set by the original and the remake. A group of military trainees stop by in the desert to check in on some scientists and find themselves run afoul of the mutant family from the first film (at least those that remain plus some new ones). There's plenty of gore to be had here. What annoys me about this film is the utter lack of characterization. The viewer does not give a damn about what happens to any of these people because we haven't gotten into them. Even the mutants had some characterization last time out and this time, nothing. Gore for the sake of gore is pointless. There has to be a reason for this to happen for it to be interesting. Nothing that happens here is interesting. And what is it lately with rape scenes in films? Here we get yet another one for no real reason. Hopefully this is one set of hills that won't be visited again.\"\n"," b'There have been some funny movies about spirits to come out of Hollywood. Cary Grant was an angel in \"The Bishop\\'s Wife\" (1947). Of course the best were the Topper movies in the late \\'30s-early \\'40s. And, more recently, Warren Beatty\\'s \"Heaven Can Wait\" (1978), which was a remake of 1941\\'s \"Here Comes Mr. Jordan.\" These were well-written, funny, entertaining comedies, all of which centered around supernatural creatures like ghosts and angels.<br /><br />Now comes writer-director Jeff Lowell, making his feature film debut with a story of an unlikable, bitchy young woman, Kate (Eva Longoria Parker), who gets killed on her wedding day and then comes back to harass the fledgling spiritualist, Ashley (Lake Bell) who is falling for Kate\\'s fianc\\xc3\\xa9, Henry (Paul Rudd). One thing that is clear at the outset: Longoria Parker is no Constance Bennett (Marion Kerby in the first two \"Topper\" films), who is the standard against whom all female ghosts are measured.<br /><br />There is a line right at the beginning when Henry\\'s sister, Chloe (Lindsay Sloane) tells Henry, \"You don\\'t smile.\" That aptly described my situation throughout this film.<br /><br />The main problem with the film is that the script just isn\\'t very funny. But it\\'s made worse by Longoria Parker\\'s presence that just rubbed me the wrong way every time she appeared on the screen. Just to start out with, compounding her lack of comedic talent, she is covered with so much pancake makeup, who knows what she really looks like? Kate gets killed while setting up for her wedding by a falling frozen statue. She\\'s so unreasonable that the angel who instructs her about what her afterlife is about walks out on her (well, she actually just fades out), so Kate finds herself back on earth as a ghost without knowing what her mission is.<br /><br />Chloe wants Henry to snap out of the funk into which he has naturally descended after Kate\\'s death (from what I saw of Kate, he should have felt a wonderful relief), so she introduces him to Ashley, who really doesn\\'t know what she\\'s doing as a spiritualist (she is also a cateress to make ends meet), to see if she can get Henry back in touch with Kate. There\\'s a lot of meshugaas that goes on.<br /><br />The vacuity of the film is epitomized by a \"B\" story revolving around Ashley\\'s assistant, Dan (Jason Biggs). This is thrown in near the end, but the way Ashley handles it indicates that she\\'s as much of a boob as Kate. Since Dan is apparently attracted to both of these severely flawed women, he deserves whatever he gets.<br /><br />Eventually Kate appears to Ashley and the fun should begin. It doesn\\'t, and more\\'s the pity because in other hands this could have been pretty funny. As it is, Norman Z. McLeod, Constance Bennett, Roland Young, Alan Mowbray, and Co. must be turning over in their graves to see this is what their brilliance in the first two \"Topper\" films has wrought.'\n"," b\"Master cin\\xc3\\xa9aste Alain Resnais likes to work with those actors who are a part of his family.In this film too we see Resnais' family members like Pierre Arditi, Sabine Azema, Andr\\xc3\\xa9 Dussolier and Fanny Ardant dealing with serious themes like death,religion,suicide,love and their overall implications on our daily lives.The formal nature of relationship shared by these people is evident as even friends, they address each other using a formal you.In 1984,while making L'amour \\xc3\\xa0 mort,Resnais dealt with time,memory and space to unravel the mysteries of a fundamental question of human existence :Is love stronger than death ? It was 16 years ago in 1968 that Resnais made a somewhat similar film Je t'aime Je t'aime which was also about love and memories.Message of this film is loud and clear :true and deep love can even put science to shame as dead lovers regain their lost lives leaving doctors to care for their reputation.L'amour \\xc3\\xa0 mort is like a game which is not at all didactic.It is a film in which the musical score is in perfect tandem with its images.This is one of the reasons why this film can easily be grasped.\"\n"," b\"I absolutely adore the 'Toxic Avenger' series, but this weak offering by the Troma people didn't make any sense, and it had me yawning all the time.<br /><br />A leaking nuclear plant (and the growing weed next to it) makes the youngsters of Tromaville High go nuts, which causes them to join a gang, have sex, explode, and whatever. Also there's some sort of monster breeding in the high school... my God, this movie's a mess.<br /><br />The actors pretty much stopped their efforts after this one and they should. The (intended) overacting started to get on my nerves in about 5 minutes...<br /><br />Disappointing. 2/10.\"], shape=(32,), dtype=string)\n","\n","\n","tf.Tensor([0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0], shape=(32,), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["**Dummy Vectorization example**\n","\n","Understand The TextVectorization layer in keras"],"metadata":{"id":"N6fnX1lA2JoD"}},{"cell_type":"code","source":["import string\n","# The TextVectorization layer is just like this class example created here\n","\n","class Vectorizer:\n","  \n","    def standardize(self, text):\n","        text = text.lower()\n","        return \"\".join(char for char in text if char not in string.punctuation)\n","\n","    def tokenize(self, text):\n","        text = self.standardize(text)\n","        return text.split()\n","\n","    def make_vocabulary(self, dataset):\n","        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n","        for text in dataset:\n","            text = self.standardize(text)\n","            tokens = self.tokenize(text)\n","            for token in tokens:\n","                if token not in self.vocabulary:\n","                    self.vocabulary[token] = len(self.vocabulary)\n","        self.inverse_vocabulary = dict(\n","            (v, k) for k, v in self.vocabulary.items())\n","\n","    def encode(self, text):\n","        text = self.standardize(text)\n","        tokens = self.tokenize(text)\n","        return [self.vocabulary.get(token, 1) for token in tokens]\n","\n","    def decode(self, int_sequence):\n","        return \" \".join(\n","            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n","\n"],"metadata":{"id":"15Cbgwfg2IxV","executionInfo":{"status":"ok","timestamp":1684917438337,"user_tz":-120,"elapsed":502,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}}},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":["Exercise"],"metadata":{"id":"CIEPnowFeEXQ"}},{"cell_type":"code","source":["\n","# initialize the class\n","<your-answer>\n","\n","\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","\n","#create the vocabulary from the dataset\n","<your-answer>\n"],"metadata":{"id":"I4xouGR92Ini","executionInfo":{"status":"ok","timestamp":1684899967574,"user_tz":-120,"elapsed":240,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"6CKFKfEqeGBV"}},{"cell_type":"code","source":["# initialize the class\n","\n","#SOL\n","vectorizer = Vectorizer()\n","\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","\n","#create the vocabulary from the dataset\n","\n","#SOL\n","vectorizer.make_vocabulary(dataset)"],"metadata":{"id":"GeaUOzANd20w","executionInfo":{"status":"ok","timestamp":1684917444095,"user_tz":-120,"elapsed":315,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["Exercise"],"metadata":{"id":"s6JrL_x8eHUo"}},{"cell_type":"code","source":["test_sentence = \"I write, rewrite, and still rewrite again\"\n","#encode the test_sentence\n","<your-answer>\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bhiyhko33UAq","executionInfo":{"status":"ok","timestamp":1684899984157,"user_tz":-120,"elapsed":218,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"933231a9-daff-41c3-fcc4-a8fb47c9590c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 3, 5, 7, 1, 5, 6]\n"]}]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"lb3c8ohVeIhA"}},{"cell_type":"code","source":["test_sentence = \"I write, rewrite, and still rewrite again\"\n","#encode the test_sentence\n","\n","#SOL\n","encoded_sentence = vectorizer.encode(test_sentence)\n","print(encoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cKX5WAQFd_VL","executionInfo":{"status":"ok","timestamp":1684917544253,"user_tz":-120,"elapsed":232,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"9f7f619e-f4cb-489a-d933-8070cf2523b7"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["[2, 3, 5, 7, 1, 5, 6]\n"]}]},{"cell_type":"markdown","source":["Exercise"],"metadata":{"id":"54Cu5RmYeSnE"}},{"cell_type":"code","source":["#decode the test_sentence\n","<your-answer>\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-o-HGoCq33Ma","executionInfo":{"status":"ok","timestamp":1684900144391,"user_tz":-120,"elapsed":238,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"10a777c8-a12e-47f5-ff12-b47afd70d133"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["i write rewrite and [UNK] rewrite again\n"]}]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"H2JdoNUJeT5O"}},{"cell_type":"code","source":["#decode the test_sentence\n","\n","#SOL\n","decoded_sentence = vectorizer.decode(encoded_sentence)\n","print(decoded_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RZLN_dVveO5p","executionInfo":{"status":"ok","timestamp":1684910178813,"user_tz":-120,"elapsed":2,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"0bf48716-e407-4bab-c8a0-b6f876c4ed65"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["i write rewrite and [UNK] rewrite again\n"]}]},{"cell_type":"markdown","source":["Optional example: if you need to pre process your text data in a specific way that the TextVectorizarion layer doesn't do in keras, you can define your own. "],"metadata":{"id":"CpcYEuwa4WqD"}},{"cell_type":"code","source":["#this will be skipped in lab class\n","from tensorflow.keras.layers import TextVectorization\n","import re\n","import string\n","import tensorflow as tf\n","\n","#initialize\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n",")\n","\n","#your custom functions\n","\n","def custom_standardization_fn(string_tensor):\n","    lowercase_string = tf.strings.lower(string_tensor)\n","    return tf.strings.regex_replace(\n","        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n","\n","def custom_split_fn(string_tensor):\n","    return tf.strings.split(string_tensor)\n","\n","#add the functions in the layer parameters\n","text_vectorization = TextVectorization(\n","    output_mode=\"int\",\n","    #added here\n","    standardize=custom_standardization_fn,\n","    #added here\n","    split=custom_split_fn,\n",")\n","\n","#check results with example\n","dataset = [\n","    \"I write, erase, rewrite\",\n","    \"Erase again, and then\",\n","    \"A poppy blooms.\",\n","]\n","text_vectorization.adapt(dataset)\n"],"metadata":{"id":"UnhTC2Xc4V67"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3wtlAuyrvQM_"},"source":["####3. Vectorizing"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"LfQ0mhpAvQM_","executionInfo":{"status":"ok","timestamp":1684918252809,"user_tz":-120,"elapsed":6424,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}}},"outputs":[],"source":["#Data Transformation\n","\n","from tensorflow.keras import layers\n","#resulting in a tensor of shape [batch_size, output_sequence_length], can be padded or truncated\n","max_length = 600\n","#The maximum size of the vocabulary for this layer.\n","max_tokens = 20000\n","#if output mode is 'tf-idf'\n","    #It weights a given term by taking “term frequency,” \n","    #how many times the term appears in the current document, \n","    #and dividing it by a measure of “document frequency,” \n","    #which estimates how often the term comes up across the dataset\n","text_vectorization = layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=max_length,\n",")\n","\n","#just like fit\n","text_vectorization.adapt(text_only_train_ds)\n","\n","int_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)"]},{"cell_type":"code","source":["for x,y in int_test_ds:\n","  print(x)\n","  print(y)\n","  break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zYutV2QH9Con","executionInfo":{"status":"ok","timestamp":1684918267867,"user_tz":-120,"elapsed":674,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"1aef8764-c935-4c91-f5d1-583828f57883"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor(\n","[[   2 1053  111 ...    0    0    0]\n"," [  10  496    2 ...    0    0    0]\n"," [ 141    8    2 ...    0    0    0]\n"," ...\n"," [   2 6304 1326 ...    0    0    0]\n"," [  11    7   33 ...    0    0    0]\n"," [  11   18    7 ...    0    0    0]], shape=(32, 600), dtype=int64)\n","tf.Tensor([0 1 1 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1], shape=(32,), dtype=int32)\n"]}]},{"cell_type":"markdown","source":["####Create Baseline model"],"metadata":{"id":"mYsRWnuh7Uqj"}},{"cell_type":"markdown","source":["Exercise"],"metadata":{"id":"iWifVMhyemwP"}},{"cell_type":"code","source":["max_tokens=20000\n","hidden_dim=16\n","\n","inputs = <your-answer> \n","x = <your-answer> #dense layer with relu\n","x = <your-answer> #dropout 0.5\n","outputs = <your-answer> #activation function is sigmoid\n","model = keras.Model(inputs, outputs)\n","<your-answer> #the optimizer is rmsprop\n"],"metadata":{"id":"hmHuu1DI7wma"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"sgf9k24Teoez"}},{"cell_type":"code","source":["#SOL\n","max_length=600\n","hidden_dim=16\n","\n","inputs = keras.Input(shape=(max_length,)) #this is wrong\n","x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model_baseline = keras.Model(inputs, outputs)\n","model_baseline.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","\n","model_baseline.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"baseline_text.keras\",\n","                                    save_best_only=True)\n","]\n","\n","#takes around 1h for the 10 epochs with the free colab version\n","model_baseline.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)\n","model_baseline = keras.models.load_model(\"baseline_text.keras\")\n","print(f\"Test acc: {model_baseline.evaluate(int_test_ds)[1]:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V_slltbx7kYf","executionInfo":{"status":"ok","timestamp":1684910336081,"user_tz":-120,"elapsed":29328,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"d0a3f664-4d94-43a2-cb18-567cc5544918"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 600)]             0         \n","                                                                 \n"," dense_2 (Dense)             (None, 16)                9616      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 16)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 17        \n","                                                                 \n","=================================================================\n","Total params: 9,633\n","Trainable params: 9,633\n","Non-trainable params: 0\n","_________________________________________________________________\n","500/500 [==============================] - 14s 22ms/step - loss: 67.3216 - accuracy: 0.4959 - val_loss: 1.5201 - val_accuracy: 0.4987\n","782/782 [==============================] - 5s 6ms/step - loss: 1.4374 - accuracy: 0.5005\n","Test acc: 0.500\n"]}]},{"cell_type":"markdown","metadata":{"id":"idGE8OAbvQNA"},"source":["####Main model (Embedding layer)"]},{"cell_type":"code","source":["#this layer takes word index and transform that into a word vector of [1,output_dim] size \n","#input=[batch_size, sequence_length]\n","#output=[batch_size,sequence_length, embedding_ dimensionality]\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n","#Bidirectional: two RNN layers running in parallel, \n","#with one processing the tokens in their natural order, \n","#and the other processing the same tokens in reverse\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model_emb = keras.Model(inputs, outputs)\n","model_emb.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model_emb.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n","                                    save_best_only=True)\n","]\n","model_emb.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)\n","model_emb = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n","print(f\"Test acc: {model_emb.evaluate(int_test_ds)[1]:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":746},"id":"jh77jSnmf-vr","executionInfo":{"status":"error","timestamp":1684921241231,"user_tz":-120,"elapsed":360955,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"46da791d-9dce-40fe-b3da-3689443f7d35"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_8\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_10 (InputLayer)       [(None, None)]            0         \n","                                                                 \n"," embedding_6 (Embedding)     (None, None, 256)         5120000   \n","                                                                 \n"," bidirectional_2 (Bidirectio  (None, 64)               73984     \n"," nal)                                                            \n","                                                                 \n"," dropout_3 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_7 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 5,194,049\n","Trainable params: 5,194,049\n","Non-trainable params: 0\n","_________________________________________________________________\n","400/400 [==============================] - ETA: 0s - loss: 0.5548 - accuracy: 0.7167"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-9eaa0914f6b5>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                                     save_best_only=True)\n\u001b[1;32m     21\u001b[0m ]\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mmodel_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_train_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint_val_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mmodel_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embeddings_bidir_gru.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test acc: {model_emb.evaluate(int_test_ds)[1]:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                         )\n\u001b[0;32m-> 1729\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2070\u001b[0m                         ):\n\u001b[1;32m   2071\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["####Export and predict model"],"metadata":{"id":"b_wWgxrs-Tob"}},{"cell_type":"markdown","source":["Exercise"],"metadata":{"id":"QGnEy03skDfU"}},{"cell_type":"code","source":["#Based on the doc: https://keras.io/examples/nlp/pretrained_word_embeddings/\n","# predict the sentence: 'This movie is complex and confusing, i didn't like it'\n","\n","<your-answer>"],"metadata":{"id":"UcOdmVackGdu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Solution"],"metadata":{"id":"NIk6u80RkFGq"}},{"cell_type":"code","source":["#https://keras.io/examples/nlp/pretrained_word_embeddings/\n","import numpy as np\n","string_input = keras.Input(shape=(1,), dtype=\"string\")\n","x = text_vectorization(string_input)\n","preds = model_emb(x)\n","end_to_end_model = keras.Model(string_input, preds)\n","\n","probabilities = end_to_end_model.predict(\n","    [[\"this movie is about computer graphics and 3D modeling and is bad\"]]\n",")\n","\n","print([np.argmax(probabilities[0])])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZmSrCOT-Sf7","executionInfo":{"status":"ok","timestamp":1684911685343,"user_tz":-120,"elapsed":3478,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"b382ff16-4040-4053-d177-facdd9fa7770"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f6498b8b640> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 2s 2s/step\n","[0]\n"]}]},{"cell_type":"code","source":["len(probabilities[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aYCODLoH_it","executionInfo":{"status":"ok","timestamp":1684921317970,"user_tz":-120,"elapsed":250,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}},"outputId":"9b124fb8-c8dc-4954-fbaf-32fb6c89af96"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["#Clustering the embedding space\n","#https://stackoverflow.com/questions/51235118/how-to-get-word-vectors-from-keras-embedding-layer\n","\n"],"metadata":{"id":"qdYld0-_9zgz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3UGpGEdQvQNB"},"source":["#### Improvement step: Using pretrained word embeddings (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yWBzrOfdvQNB"},"outputs":[],"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sruGts72vQNB"},"outputs":[],"source":["import numpy as np\n","path_to_glove_file = \"glove.6B.100d.txt\"\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(f\"Found {len(embeddings_index)} word vectors.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOOshtdTvQNB"},"outputs":[],"source":["embedding_dim = 100\n","\n","vocabulary = text_vectorization.get_vocabulary()\n","word_index = dict(zip(vocabulary, range(len(vocabulary))))\n","\n","embedding_matrix = np.zeros((max_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    if i < max_tokens:\n","        embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ezhgDWgvQNB"},"outputs":[],"source":["embedding_layer = layers.Embedding(\n","    max_tokens,\n","    embedding_dim,\n","    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","    trainable=False,\n","    mask_zero=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xawZ8yCzvQNB"},"outputs":[],"source":["inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = embedding_layer(inputs)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n","print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","source":["####Main model 2: Transformer for text classification"],"metadata":{"id":"k0ds5eiabjao"}},{"cell_type":"code","source":["class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=input_dim, output_dim=output_dim)\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=output_dim)\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","            \"input_dim\": self.input_dim,\n","        })\n","        return config"],"metadata":{"id":"-lUuaz-ckqpr","executionInfo":{"status":"ok","timestamp":1684911885490,"user_tz":-120,"elapsed":2,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim)\n","        self.dense_proj = keras.Sequential(\n","            [layers.Dense(dense_dim, activation=\"relu\"),\n","             layers.Dense(embed_dim),]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","\n","    def call(self, inputs, mask=None):\n","        if mask is not None:\n","            mask = mask[:, tf.newaxis, :]\n","        attention_output = self.attention(\n","            inputs, inputs, attention_mask=mask)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"num_heads\": self.num_heads,\n","            \"dense_dim\": self.dense_dim,\n","        })\n","        return config"],"metadata":{"id":"8_90QtVlkrll","executionInfo":{"status":"ok","timestamp":1684911885831,"user_tz":-120,"elapsed":0,"user":{"displayName":"Catarina Pereira","userId":"01889833079344592364"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["\n","\n","vocab_size = 20000\n","sequence_length = 600\n","embed_dim = 256\n","num_heads = 2\n","dense_dim = 32\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","#order\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n","#context\n","x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","x = layers.GlobalMaxPooling1D()(x)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)\n","model = keras.models.load_model(\n","    \"full_transformer_encoder.keras\",\n","    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n","                    \"PositionalEmbedding\": PositionalEmbedding})\n","print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1-rVDl4xbidp","outputId":"814c80fd-c2d1-41ad-d829-56714a639af2"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Model: \"model_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_9 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," positional_embedding (Posit  (None, None, 256)        5273600   \n"," ionalEmbedding)                                                 \n","                                                                 \n"," transformer_encoder (Transf  (None, None, 256)        543776    \n"," ormerEncoder)                                                   \n","                                                                 \n"," global_max_pooling1d (Globa  (None, 256)              0         \n"," lMaxPooling1D)                                                  \n","                                                                 \n"," dropout_2 (Dropout)         (None, 256)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 1)                 257       \n","                                                                 \n","=================================================================\n","Total params: 5,817,633\n","Trainable params: 5,817,633\n","Non-trainable params: 0\n","_________________________________________________________________\n","400/400 [==============================] - ETA: 0s - loss: 0.6313 - accuracy: 0.6862"]}]},{"cell_type":"code","source":["#https://arxiv.org/abs/1907.11692\n","#https://github.com/keras-team/keras-nlp/blob/v0.5.1/keras_nlp/models/roberta/roberta_backbone.py#L36"],"metadata":{"id":"QH9qU1VQSnqL"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part02_sequence-models.ipynb","timestamp":1684898264635}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"nbformat":4,"nbformat_minor":0}