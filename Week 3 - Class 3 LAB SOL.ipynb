{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd64156",
   "metadata": {},
   "source": [
    "# Week 3 - Class 3\n",
    "\n",
    "This notebook have the most important exercises data wrangling operations for the data used in this course\n",
    "\n",
    "- Text\n",
    "- Numbers\n",
    "- Images Segmentation\n",
    "- Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5512175",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da184482",
   "metadata": {},
   "source": [
    "Your dataset information:\n",
    "\n",
    "https://keras.io/api/datasets/imdb/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45a2caf",
   "metadata": {},
   "source": [
    "dataset of 25,000 movies reviews from IMDB\n",
    "abeled by sentiment (positive/negative): see set(train_labels)\n",
    "each review is encoded as a list of word indexes (integers): see train_data[0] for example\n",
    "\n",
    "the integer \"5\" encodes the 5th most frequent word in the data\n",
    "## Zero (0)\n",
    "\"0\" does not stand for a specific word, but instead is used to encode the pad token\n",
    "Padding is add the zeros at the end of the sequence to make the samples in the same size\n",
    "\n",
    "maxlen: int or None. Maximum sequence length. Any longer sequence will be truncated. Defaults to None, which means no truncation. None== length of the longest sentence, then truncation never happens\n",
    "\n",
    "## One (1)\n",
    "Start of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a32efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "# These are the default parameters\n",
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 3 #start_char+oov_char\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
    "    num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d04665",
   "metadata": {},
   "source": [
    "The '3-index shift issue': see the load_data() and the get_word_index() as two separate functions. \n",
    "\n",
    "The load_data has special parameters, such as having reserved the 1 and 2 indexes for start_char and oov_char, respectively. \n",
    "\n",
    "The get_word_index gives the indexes of the words by overall frequency in the dataset, being the 1, the most frequent word in the dataset. This is independent from the load_data() function. \n",
    "\n",
    "When we load the data, we can choose to drop x-first indexes, the most frequent words. With a index_from, are saying to the function: 'give me the indexes of words shift +3'. Because of this, when converting the index to words, we need to subtract 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688ca973",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 0 \n",
    "\n",
    "(x_train0, _), _ = imdb.load_data(start_char=start_char, oov_char=oov_char, index_from=index_from)\n",
    "\n",
    "index_from = 3 #re assigning \n",
    "(x_train3, _), _ = imdb.load_data(start_char=start_char, oov_char=oov_char, index_from=index_from)\n",
    "\n",
    "print('zero:',x_train0[0][0:4]) #print first sample of train, for the first 5 words\n",
    "print('3-indx:',x_train3[0][0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c622b22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = 1\n",
    "oov_char = 2\n",
    "index_from = 0 #start_char+oov_char\n",
    "\n",
    "(x_train, _), _ = imdb.load_data(start_char=start_char, oov_char=oov_char, index_from=index_from)\n",
    "\n",
    "x_train[0][0:4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adaab85",
   "metadata": {},
   "source": [
    "To obtain the words, from the word index, we can subtract 3 from the train data index, or sum 3 from the word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0648f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = imdb.get_word_index()\n",
    "# see the word index sorted\n",
    "#{k: v for k, v in sorted(word_index.items(), key=lambda item: item[1])}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f24960",
   "metadata": {},
   "source": [
    "### Sum to word index example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c4e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the word index file mapping words to indices\n",
    "word_index = imdb.get_word_index()\n",
    "# Reverse the word index to obtain a dict mapping indices to words\n",
    "# And add `index_from` to indices to sync with `x_train`\n",
    "inverted_word_index = dict(\n",
    "    (i + index_from, word) for (word, i) in word_index.items()\n",
    ")\n",
    "# Update `inverted_word_index` to include `start_char` and `oov_char`\n",
    "inverted_word_index[start_char] = \"[START]\"\n",
    "inverted_word_index[oov_char] = \"[OOV]\"\n",
    "# Decode the first sequence in the dataset\n",
    "decoded_sequence = \" \".join(inverted_word_index[i] for i in x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0216eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe9fc41",
   "metadata": {},
   "source": [
    "### Substract to train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074c5036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of samples in training and testing dataset\n",
    "\n",
    "print(train_data.shape,train_labels.shape)\n",
    "print(test_data.shape,test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the size of training and testing dataset for one sample\n",
    "\n",
    "print(len(train_data[1]), train_labels[0])\n",
    "print(len(test_data[1]), test_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7ed85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the maximum length in the data\n",
    "\n",
    "max([max(sequence) for sequence in train_data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744763e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#create a function to transfor list of word-indexes to vectors\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    #start by create the placeholder for the word vectors\n",
    "    #np.zeros(rows, columns)-> array with zeros with rows=nb of samples and columns=vocabulary dimension\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    #for each row, give an i (index) and the sequence itself\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        #inside the sequence, j will go through the columns\n",
    "        for j in sequence:\n",
    "            #for i (the sequence number) and j (the column in that sequence), assign 1 to that cell in results\n",
    "            #results for row i, in j column will have a 1, if sequence i has j in its values\n",
    "            results[i, j] = 1.\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e90040",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66543ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072f1b4",
   "metadata": {},
   "source": [
    "# Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fdf5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3983c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6c3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4556c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534c0e9",
   "metadata": {},
   "source": [
    "# Images Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4c6e85",
   "metadata": {},
   "source": [
    "Get the images from these two links and extract them:\n",
    "http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131c510",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e639b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
    "!tar -xf images.tar.gz\n",
    "!tar -xf annotations.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a448c22",
   "metadata": {},
   "source": [
    "Do a list of images paths and of target paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f421752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_dir = \"images/\"\n",
    "target_dir = \"annotations/trimaps/\"\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [os.path.join(input_dir, fname)\n",
    "     for fname in os.listdir(input_dir)\n",
    "     if fname.endswith(\".jpg\")])\n",
    "target_paths = sorted(\n",
    "    [os.path.join(target_dir, fname)\n",
    "     for fname in os.listdir(target_dir)\n",
    "     if fname.endswith(\".png\") and not fname.startswith(\".\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17695ba",
   "metadata": {},
   "source": [
    "Plot the image 10th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42bb269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(load_img(input_img_paths[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806d9a75",
   "metadata": {},
   "source": [
    "Plot its target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9d2d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_target(target_array):\n",
    "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(normalized_array[:, :, 0])\n",
    "\n",
    "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
    "display_target(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc92f073",
   "metadata": {},
   "source": [
    "How to load an image to a numeric tensor!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a56eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "img_size = (200, 200)\n",
    "num_imgs = len(input_img_paths)\n",
    "\n",
    "random.Random(1337).shuffle(input_img_paths)\n",
    "random.Random(1337).shuffle(target_paths)\n",
    "\n",
    "def path_to_input_image(path):\n",
    "    return img_to_array(load_img(path, target_size=img_size))\n",
    "\n",
    "def path_to_target(path):\n",
    "    img = img_to_array(\n",
    "        load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
    "    img = img.astype(\"uint8\") - 1\n",
    "    return img\n",
    "\n",
    "input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\n",
    "targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n",
    "for i in range(num_imgs):\n",
    "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
    "    targets[i] = path_to_target(target_paths[i])\n",
    "\n",
    "num_val_samples = 1000\n",
    "train_input_imgs = input_imgs[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_input_imgs = input_imgs[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7da3c7",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle competitions download -c dogs-vs-cats\n",
    "!unzip -qq dogs-vs-cats.zip\n",
    "!unzip -qq train.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf18c27",
   "metadata": {},
   "source": [
    "Create directories and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ca3fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, pathlib\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "original_dir = pathlib.Path(\"train\")\n",
    "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
    "\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname,\n",
    "                            dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de2f9fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d33a1388bb95f6f47390937b44de6829ea073fadb3ed2a95296fdc5e4fdfba80"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
