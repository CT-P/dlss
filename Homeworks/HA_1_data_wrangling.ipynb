{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1: Data wrangling with python\n",
        "\n",
        "This assignemnet focus on text as data, with groupby, apply, and text vectorization focus.\n",
        "\n",
        "We will work with this dataset: http://groups.di.unipi.it/~gulli/AG_corpus_of_news_articles.html"
      ],
      "metadata": {
        "id": "S8U-JvQALsIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#get the dataset for the current directory of this notebook\n",
        "!wget https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n"
      ],
      "metadata": {
        "id": "qsPQlHaQqLs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#visualize the data\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"train.csv\", header=None)\n",
        "df.info()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "EuHmDI6lqWJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See how the apply is transforming the dataset\n",
        "df.columns = [\"label\", \"title\", \"lead\"]\n",
        "label_map = {1:\"world\", 2:\"sport\", 3:\"business\", 4:\"sci/tech\"}\n",
        "def replace_label(x):\n",
        "\treturn label_map[x]\n",
        "df[\"label\"] = df[\"label\"].apply(replace_label) "
      ],
      "metadata": {
        "id": "MP_H9FpDqzJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Print first rows\n",
        "df.head()"
      ],
      "metadata": {
        "id": "JI1FyObEsXSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 (15 points)\n",
        "\n",
        "implement a new column text which contains the lowercased title and lead"
      ],
      "metadata": {
        "id": "1mQzLHZM4_hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO implement a new column text which contains the lowercased title and lead\n"
      ],
      "metadata": {
        "id": "6SdMdF91r8An"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2 (15 points)\n",
        "\n",
        "print the number of documents for each label"
      ],
      "metadata": {
        "id": "X0liWjYX5Foz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO print the number of documents for each label\n"
      ],
      "metadata": {
        "id": "I_j5Ye7WycB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3 (15 points)\n",
        "\n",
        "Create a new column with the number of words for each text"
      ],
      "metadata": {
        "id": "DCX6yNF55HuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO create a new column with the number of words for each text\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lkB5pRGWxMk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SOLUTION\n",
        "df['words_text']= df.text.apply(lambda x: len(str(x).split(' ')))"
      ],
      "metadata": {
        "id": "2K6dRU_VMlka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4 (15 points)\n",
        "\n",
        "plot the average number of words per label "
      ],
      "metadata": {
        "id": "BbD0mW9b5LZL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO plot the average number of words per label \n",
        "# tip: https://queirozf.com/entries/pandas-dataframe-plot-examples-with-matplotlib-pyplot\n"
      ],
      "metadata": {
        "id": "2AhlsmZB5Klq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5 (15 points)\n",
        "\n",
        "implement a function which counts how often a pattern appears in a text"
      ],
      "metadata": {
        "id": "WTgNvWrx5Pab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-Dy_J3dp4qZ"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "keywords = [\"switzerland\", \"usa\", \"europe\", \"israel\"]\n",
        "for keyword in keywords:\n",
        "    print(keyword)\n",
        "    \n",
        "    def count_keyword_frequencies(x):\n",
        "        #TODO implement a function which counts how often a pattern appears in a text\n",
        "        num_occurrences = x.count(keyword)\n",
        "        return num_occurrences\n",
        "    # Now, we can print how often a keyword appears in the data\n",
        "    print (df[\"text\"].apply(count_keyword_frequencies).sum())\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6 (15 points)\n",
        "\n",
        "Create a vectorization of the dataset using the SKlearn CountVectorizer with the full vocabulary\n",
        "\n",
        "How long is the vocabulary for the whole dataset?\n",
        "\n",
        "tip: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ],
      "metadata": {
        "id": "QHLBflqMLaxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Create a vectorization of the dataset using the SKlearn CountVectorizer with the full vocabulary\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P6vLRHaMOCXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: How long is the vocabulary for the whole dataset?\n",
        "\n"
      ],
      "metadata": {
        "id": "UiG7HOpWPYj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7 (15 Points)\n",
        "\n",
        "Separate the vectorized dataset into training and testing dataset with a test size of 30%"
      ],
      "metadata": {
        "id": "PBZwzwj0LSkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Separate the vectorized dataset into training and testing dataset with a test size of 30%\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZjsqUvECNORt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deliver in HTML"
      ],
      "metadata": {
        "id": "l6BnoaK-LoFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HOW TO DELIVER: https://stackoverflow.com/questions/53460051/convert-ipynb-notebook-to-html-in-google-colab\n",
        "#download this notebook after all your answers and upload it to the files tab on the left\n",
        "#right click on it to check the path \n",
        "#replace the path ('/content/Untitled0.ipynb') by your file path\n",
        "%%shell\n",
        "jupyter nbconvert --to html /content/Untitled0.ipynb\n",
        "\n",
        "#refresh the folder (not the page)\n",
        "#download the html"
      ],
      "metadata": {
        "id": "Lv6dpCVu3Uho"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}